## Running

*Do NOT run this test suite against a production endpoint OR where all statements can't be voided.*

The test suite is broken into two stages to allow for eventual consistency LRS implementations. These stages can be run independently and have some of their own configuration properties. Stage1 runs requests that either do not offer eventual consistency or that are used to store statements that will be checked once the LRS is consistent. Stage2 leverages files stored on the filesystem by the first stage to make requests to the LRS after determining whether it is consistent.

Along with the tests themselves there are additional maintenance capabilities. Including the ability to "reset" the LRS, so that it has only voided statements and no test documents. Additionally there is a "clean" capability which will remove all automatically generated files in the local directory.

## Dependencies

This test suite requires Node.js, grunt, and access to an LRS.

To install grunt use `npm`, you may need escalated privileges via `sudo`, a Windows administrator shell, or similar:

    npm install -g grunt-cli

## Setup

The easiest way to run the test suite is to clone the git repository directly using:

    git clone https://github.com/adlnet/xAPI_LRS_Test

Then change to the `src/` directory:

    cd src/

With the repository cloned install the dependencies using:

    npm install

With dependencies ready, configure the LRS to be tested by copying the `config.json.template` to `config.json` and set the values of "endpoint", "username", "password", and "version".

After configuration you can run the suite using:

    grunt

## Configuration

The configuration file allows configuration of the various parameters passed to the test runner as well as stage specific parameters. By default the test suite will look for a `config.json` file in the `src/` directory. You can point to a custom configuration file by using the `--config` command line switch. The value passed to that switch can be either an absolute path or a path relative to the `src/` directory. For example the following commands would use the same configuration file:

    grunt --config="config-my-test-suite.json"
    grunt --config="/path/to/my/repo/src/config-my-test-suite.json"

If you do not already have a config.json file in place, you can set basic lrs options via command line and grunt will create the config.json file for you from the template. To do this, execute the following command with no config.json file in place.

    grunt --endpoint=http://endpoint.lrs --username=user1 --password=pwd1 --xapi-version=1.0.0

If you do not have a config.json file and do not provide the options, grunt will fail and require you to create the config file before continuing.

The repository is setup to ignore files matching `config*.json`.

### General Configuration

* `lrs` is an object that contains the values necessary to connect to the LRS to be tested
    * `endpoint`
    * `username`
    * `password`
    * `version`
* `bail` indicates that the test suite should stop when it encounters the first failed test, defaults to 'false' (or off)
* `reporter` controls which reporter is used to output the test results, defaults to 'spec' (see `node_modules/.bin/mocha --reporters`)
* `timeout` controls how long in ms a scenario is allowed to take before it is marked failing, defaults to 10000
* `slow` controls how long in ms a step can take before it is indicated as being slow, defaults to 1000
* `grep` allows for a regular expression to select specific scenario(s) to be run
* `persistence` controls various settings for the storage of temporary files, and to read adhoc files
    * `statementStore` path where temporary files will be stored for use during stage2 statement structure validation, defaults to 'var/statements/'
    * `statementRead` array of paths to read for statements to be fetched and matched against the LRS includes `statementStore` automatically
    * `adhocValid` array of paths to read for files containing presumed good statements to be sent to the LRS
    * `adhocInvalid` array of paths to read for files containing presumed bad statements to be sent to the LRS
    * `conflicts` path where temporary files are stored for stage2 conflict validation, defaults to 'var/conflicts/'
    * `queries` path where temporary files are stored for stage2 stream query validation, defaults to 'var/queries/'
* `diagnostics` controls various settings for the output of the suite, takes an object with the following parameters:
    * `requestCount` turns on reporting of a summary of the requests made, defaults to 'false' (or off)
    * `stepHash` turns on output of a scenario identifier generated by hashing the step content, defaults to 'false' (or off)
* `developer` indicates that the runner of the test suite is also a developer of it, optional, defaults to 'false' (or off)

### Stage 1 Configuration

* `featureSpec` takes a string glob controlling which feature files will be run (see command examples below)
* `pending` takes an object that allows tests to be marked pending while fixing or developing an LRS, the keys of the object are strings that will be output in the scenario title to assist knowing why a test is pending, the values are arrays containing string hashes of the scenarios that should be marked pending a result of the key

    For example:

        ```json
        {
            "stage1": {
                "pending": {
                    "Issue #314: Fix SubStatement validation": [
                        "de30eb93ebc194739752be9365cac0e1",
                        "fdac2f4b5aa5c97e50075279145d8e22"
                    ]
                }
            }
        }
        ```

* `stalePending` takes a boolean and when true turns on output of a list of "stale" hashes in the pending list but only when the command line 'feature[s]' flag is not used

### Stage 2 Configuration

* `queries` takes an object with stream query specific configuration parameters
    * `pending` takes a value as described above under Stage 1 pending, but for stream query scenarios

## Commands

There are a number of different ways to run part of the test suite, and to target specific features to test.

### Tasks

The following are the most common commands:

    > grunt
    > grunt stage1
    > grunt stage2
    > grunt adhoc
    > grunt conflict
    > grunt query
    > grunt primeLRS
    > grunt reset
    > grunt clean
    > grunt clearSandbox

### Flags

The following flags can be used to control how the test suite executes and its output. They control the configuration options listed above.

    --bail
    --reporter
    --timeout
    --slow
    --grep
    --diagnostics (enables all diagnostics)
    --count (see diagnostics.requestCount)
    --hash (see diagnostics.stepHash)
    --feature(s) (see stage1.featureSpec)

### Example Commands

    > grunt --reporter=nyan
    > grunt --feature=features/about.feature
    > grunt stage1 --feature=features/statementStructure/
    > grunt stage1 --features=features/statementStructure/object*.feature
    > grunt stage1 --features=features/*.feature --timeout=10000 --slow=2000
    > grunt --feature=features/about.feature --count=true --hash=true
    > grunt --feature=features/about.feature --diagnostics
    > grunt --feature=features/about.feature --grep="382a1f02f68100fbddd10e9ab7e0c954" --hash
    > grunt --feature=features/about.feature --grep="382a1f02f68100fbddd10e9ab7e0c954" --diagnostics
    > grunt --feature=features/about.feature --grep="HTTP 200"

## Notes

* In order to prevent detection of non-specifically primed statements the stream query tests are all based on time bounded queries making use of the "since" and "until" query parameters. An LRS without "since" and "until" support for the statement stream will likely fail all of the stream query scenarios.
